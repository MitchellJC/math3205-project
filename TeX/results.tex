% Runtime specifications
Models were run on a Windows 11 computer with a AMD Ryzen 5 5625U 2.3GHz processor with 16 GB of RAM\@. Gurobi version 10.0.1 was used with Python version 3.9.16. 

We aimed to compare 7 models: The pure MIP, the network model, an iterative LBBD1 (iLBBD1), LBBD1 using lazy constraints (cLBBD1), iterative LBBD2 with propagation (iLBBD2p), LBBD2 with lazy constraints and propagation (cLBBD2p) and LBBD4 with lazy constraints and propagation (cLBBD4). The pure MIP was used as a baseline.\ iLBBD1 and cLBBD1  were chosen to be used as baseline LBBD models. LBBD2 with propagation was chosen as it was one of the best performers in the original paper, we intended to use it as the main comparison to the network model and our new cut. We chose to collect results for LBBD4 only using propagation and lazy constraints as we believed this would give us the best results while adhering to the project time constraints.

We generate 5 seeded instances of data similarly to~\cite{roshanaei2017propagating}. The data is generated for 3 hospitals and 5 operating rooms over a 5 day planning horizon for varying number of patients. Unlike~\cite{roshanaei2017propagating} who generate a set of data for 3 and 5 operating rooms while only varying the surgery times in each instance, we generate each instance with entirely different data. This was believed to give results more indicative of performance on a wider variation in problem parameters. Also unlike in the original paper, where models were ran with a time limit of 7200 seconds, we set our time limit to 900 seconds due to project time constraints. At a time limit of 7200 seconds the worst case time to run all models over all 5 instances was 70 hours for only one patient size. For this same reason we only ran models with 5 available operating rooms, instead of testing 5 and 3. We chose to use 5 operating rooms, this should have led to harder sub problems and relatively easier master problems compared to using 3 operating rooms\cite{roshanaei2017propagating}. 

There was uncertainty as to whether the models were solved to true optimality or to a relative MIP gap of $1\%$ as was done by~\cite{guo}, so we report results for both scenarios. We follow~\cite{roshanaei2017propagating} by defining the best performing model to be the most robust, that is, the one able to solve the model to optimality within the given time constraints while breaking ties by considering the fastest models averaged over solved instances.

The 7 models were solved to optimality on each of the 5 instances, the average time to solve can be seen in Table~\ref{tab:avgTimeOpt} and the average gaps can be seen in Table~\ref{tab:avgGapOpt}.

\input{results/avg_time_optimal.tex}

We can see from Table~\ref{tab:avgTimeOpt} that all models struggled solving to optimality for number of patients greater than only 20. The pure MIP is the most robust across all patient sizes while the Network model is the least. We can see that for sizes where LBBD models do solve an instance, they do so on average faster than the pure MIP.\@ For example, for a patient size of 20 cLBBD4p performed the best, being solved in the least time on average, with cLBBD2p being close second. For a patient size of 60, most LBBD models solved faster than the pure MIP.\@ Comparing between LBBD models, all using lazy constraints outperform those using iteration in terms of time to solve for every number of patients. The results shown in Table~\ref{tab:avgTimeOpt} suggest that none of the 7 models scale well for solving problems with sets of patients as large or larger than 60 when solving to optimality. No models were able to solve instances with 80 patients to optimality. 

\input{results/avg_gaps_optimal.tex}

Table~\ref{tab:avgGapOpt} shows that although not many instances solved to optimality, by the end of the prescribed time limit, the optimality gaps were very small. All optimality gaps had reached at least 1\% or lower. In fact, increasing the number of patients did not appear to have a large negative effect on the final optimality gaps, giving merit to the scalability of all models in term of achieving a small gap in under 15 minutes for increasing number of patients. Interestingly, nearly all gaps for number of patients greater than 20 were of the same magnitude, it seems there is a is some threshold that the model can reach very quickly but closing the last slice of gap becomes very difficult. These results gave more motivation to investigate solving to a 1\% gap.

\input{results/avg_time_optimal(1_gap).tex}

The 7 models were solved to a 1\% gap on each of the 5 instances, the average time to solve can be seen in Table~\ref{tab:avgTimeOpt1Perc} and the average gaps can be seen in Table~\ref{tab:avgTimeOpt1Perc}.

\input{results/avg_gaps_optimal(1_gap).tex}